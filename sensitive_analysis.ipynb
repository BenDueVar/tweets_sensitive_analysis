{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets - Sentiment Analysis\n",
    "\n",
    "### Loading and Exploring Data\n",
    "The dataset was downloaded from [Kaggle](https://www.kaggle.com/kazanova/sentiment140)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Sentiment140.csv',encoding='latin-1')\n",
    "sample = df.sample(n=10000, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1131005</th>\n",
       "      <td>4</td>\n",
       "      <td>1975786807</td>\n",
       "      <td>Sat May 30 15:54:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>veronicalynn</td>\n",
       "      <td>@kimberliea fail  I didn't even see him while ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922138</th>\n",
       "      <td>4</td>\n",
       "      <td>1754352859</td>\n",
       "      <td>Sun May 10 05:58:26 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>SharonDV</td>\n",
       "      <td>@Journeywoman Thank you M.E.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64766</th>\n",
       "      <td>0</td>\n",
       "      <td>1688928213</td>\n",
       "      <td>Sun May 03 12:14:08 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Emmetts</td>\n",
       "      <td>Ahw, Wipeout-Zacharias' dialect made me want t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577166</th>\n",
       "      <td>4</td>\n",
       "      <td>2189644041</td>\n",
       "      <td>Tue Jun 16 00:32:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Ellen_Stafford</td>\n",
       "      <td>@OfficialVernonK Don't do it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160252</th>\n",
       "      <td>0</td>\n",
       "      <td>1957021224</td>\n",
       "      <td>Thu May 28 23:17:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>la_oooo_ra</td>\n",
       "      <td>@mysticnz no im not  cries LOL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                          date      flag  \\\n",
       "1131005       4  1975786807  Sat May 30 15:54:07 PDT 2009  NO_QUERY   \n",
       "922138        4  1754352859  Sun May 10 05:58:26 PDT 2009  NO_QUERY   \n",
       "64766         0  1688928213  Sun May 03 12:14:08 PDT 2009  NO_QUERY   \n",
       "1577166       4  2189644041  Tue Jun 16 00:32:07 PDT 2009  NO_QUERY   \n",
       "160252        0  1957021224  Thu May 28 23:17:56 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \n",
       "1131005    veronicalynn  @kimberliea fail  I didn't even see him while ...  \n",
       "922138         SharonDV                      @Journeywoman Thank you M.E.   \n",
       "64766           Emmetts  Ahw, Wipeout-Zacharias' dialect made me want t...  \n",
       "1577166  Ellen_Stafford                      @OfficialVernonK Don't do it   \n",
       "160252       la_oooo_ra                     @mysticnz no im not  cries LOL  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.columns =['target','id','date','flag','user','text']\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data\n",
    "\n",
    "Processing urls and punctuation marks, cleaning spaces, tokenize, lemmatize and stopwords control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {\n",
    "    \":)\": \"happy\",\n",
    "    \":(\": \"sad\",\n",
    "    \":D\": \"laugh\",\n",
    "    \":'(\": \"cry\",\n",
    "    \":P\": \"playful\",\n",
    "    \";)\": \"wink\",\n",
    "    \":-/\": \"skeptical\",\n",
    "    \":-|\": \"neutral\",\n",
    "    \"<3\": \"love\"\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "def emoji_to_text(s, emoji_dict):\n",
    "    for emoji, word in emoji_dict.items():\n",
    "        s = s.replace(emoji, ' ' + word + ' ')\n",
    "    return s\n",
    "\n",
    "def pre_processing(s):\n",
    "    # Convert emojis to text\n",
    "    s = emoji_to_text(s, emoji_dict)\n",
    "    \n",
    "    # Handle negations\n",
    "    negation_words = ['not', 'no', 'never', 'none', 'nobody', 'nothing', 'neither', 'nowhere', 'hardly', 'scarcely', 'barely', 'doesn’t', 'isn’t', 'wasn’t', 'shouldn’t', 'wouldn’t', 'couldn’t', 'won’t', 'can’t', 'don’t']\n",
    "    for negation in negation_words:\n",
    "        s = re.sub(r'\\b({})\\b[\\s]?([a-z]+)'.format(negation), r'\\1_\\2', s)\n",
    "    \n",
    "    # Remove URLs\n",
    "    s = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', ' ', s)\n",
    "    \n",
    "    # Replace various punctuation marks with spaces\n",
    "    s = s.translate({ord(c): \" \" for c in \"!@#$%^&*()[]{}'\\\";:,./<>?\\|`-~=_+\"})\n",
    "    \n",
    "    # Remove digits\n",
    "    s = re.sub('\\d+', ' ', s)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(s)\n",
    "    \n",
    "    # Lemmatize\n",
    "    lm = WordNetLemmatizer()\n",
    "    lemmatized = [lm.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    l = [word for word in lemmatized if not word in stopwords.words('english')]\n",
    "    \n",
    "    return l\n",
    "\n",
    "# Apply pre-processing to the text\n",
    "sample[\"text_processed\"] = sample[\"text\"].apply(pre_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Bag of Words\n",
    "\n",
    "1. Calculate the frequency distribution of all words\n",
    "2. Then select the top 5,000 words from the frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = [word for sublist in sample[\"text_processed\"] for word in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Features\n",
    "\n",
    "Create a 2-dimensional matrix to record whether each of those words is contained in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n",
    "X_tfidf = tfidf_vectorizer.fit_transform([' '.join(tokens) for tokens in sample[\"text_processed\"]])\n",
    "y = sample['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 67265 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD APPROACH\n",
    "\n",
    "# from nltk.probability import FreqDist\n",
    "# list_of_words = sample[\"text_processed\"].tolist()\n",
    "# list_of_words\n",
    "\n",
    "# bow = []\n",
    "# for lists in list_of_words:\n",
    "#     for word in lists:\n",
    "#         bow.append(word)\n",
    "\n",
    "# freq_dist = FreqDist(bow)\n",
    "# top_5000 = freq_dist.most_common(5000)\n",
    "# word_features, freq = [[x for x,y in top_5000],[y for x,y in top_5000]]\n",
    "# def find_features(lst, bow):\n",
    "#     word_features=list(bow)\n",
    "#     words = set(lst)\n",
    "#     features = {}\n",
    "#     for w in word_features:\n",
    "#         features[w] = (w in words)\n",
    "#     return features\n",
    "# featuresets = [(find_features(rev, word_features), category) for (rev, category) in features]\n",
    "# features=[]\n",
    "# for i,l in enumerate(sample[\"text_processed\"]):\n",
    "#     s=[find_features(l,bow),sample[\"target\"].iloc[i]]\n",
    "#     z=tuple(s)\n",
    "#     features.append(z)\n",
    "# len(features)\n",
    "# train_set, test_set = features[5000:], features[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model\n",
    "\n",
    "Build and train a  Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0:\n",
      "  back: 0.0025879912058247674\n",
      "  today: 0.0026474110657255818\n",
      "  sad: 0.0026963151574641354\n",
      "  know: 0.002731645401005582\n",
      "  go: 0.00276979828664277\n",
      "  work: 0.002965482338330415\n",
      "  get: 0.0030073246776280327\n",
      "  wa: 0.003124331834415638\n",
      "  miss: 0.003207866887344859\n",
      "  day: 0.0033570211690197374\n",
      "\n",
      "Class 1:\n",
      "  new: 0.00241343360646553\n",
      "  like: 0.0025496777149691353\n",
      "  wa: 0.00257810698863464\n",
      "  lol: 0.00268083096377787\n",
      "  going: 0.0027181794239409516\n",
      "  day: 0.002960784211495757\n",
      "  quot: 0.0032696098595671953\n",
      "  thanks: 0.0037189461493811254\n",
      "  love: 0.004400246122089488\n",
      "  good: 0.004515414053264746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the feature names from the vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the log probability of each feature given a class\n",
    "feature_log_probs = classifier.feature_log_prob_\n",
    "\n",
    "# For each class, find the indices of the features with the highest log probabilities\n",
    "most_informative_features_per_class = np.argsort(feature_log_probs, axis=1)[:, -10:]  # Get top 10 features for each class\n",
    "\n",
    "# Display the most informative features for each class\n",
    "for index, class_features in enumerate(most_informative_features_per_class):\n",
    "    print(f\"Class {index}:\")\n",
    "    for feature_index in class_features:\n",
    "        print(f\"  {feature_names[feature_index]}: {np.exp(feature_log_probs[index, feature_index])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Naive Bayes Model\n",
    "\n",
    "*OK = accuracy score is over 0.6. <br>\n",
    "Good = accuracy score is over 0.7*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7068\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline \n",
    "\n",
    "Putting all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Custom transformer to apply the pre-processing\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, emoji_dict):\n",
    "        self.emoji_dict = emoji_dict\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return [self.pre_processing(text) for text in X]\n",
    "    \n",
    "    def pre_processing(self, s):\n",
    "        # Convert emojis to text\n",
    "        s = emoji_to_text(s, self.emoji_dict)\n",
    "        # ... (rest of your pre-processing code)\n",
    "        return ' '.join(l)  # Return the processed text as a single string\n",
    "\n",
    "# Define the emoji dictionary\n",
    "emoji_dict = {\n",
    "    \":)\": \"happy\",\n",
    "    # ... (rest of your emoji mappings)\n",
    "}\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor(emoji_dict)),\n",
    "    ('vectorizer', TfidfVectorizer(max_features=5000, ngram_range=(1, 3))),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Sentiment140.csv', encoding='latin-1')\n",
    "sample = df.sample(n=10000, axis=0)\n",
    "sample.columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample['text'], sample['target'], test_size=0.5, random_state=42)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
